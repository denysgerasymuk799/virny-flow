stages:
    - scheduled
    - test

build-docker-image:
    image: docker:latest

    stage: scheduled

    services:
    - docker:dind

    only:
        variables:
            - $AM_TASK == 'IMAGE'

    variables:
        DOCKER_HOST: tcp://docker:2375
        DOCKER_TLS_CERTDIR: ""
        GIT_SUBMODULE_STRATEGY: recursive

    script:
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN registry.gitlab.com
    - docker build --build-arg GITHUB_USERNAME=$GITHUB_USERNAME --build-arg GITHUB_PASSWORD=$GITHUB_PASSWORD -t registry.gitlab.com/einblick/alpine-meadow:latest -f docker/dockerfile .
    - docker push registry.gitlab.com/einblick/alpine-meadow:latest


test:
    image:
        name: registry.gitlab.com/einblick/alpine-meadow:latest
        entrypoint: [""]

    stage: test

    except:
    - schedules

    variables:
        GIT_SUBMODULE_STRATEGY: recursive

    script:
    # install
    - pip install --use-feature=in-tree-build .

    # linter
    - ./tools/build-support/run_linter.sh

    # test
    - python -m pytest -s -v --durations=0 -ra tests

mini-benchmark:
    image:
        name: registry.gitlab.com/einblick/alpine-meadow:latest
        entrypoint: [""]

    stage: scheduled

    only:
        variables:
            - $AM_TASK == 'MINI_BENCHMARK'

    variables:
        GIT_SUBMODULE_STRATEGY: recursive

    artifacts:
        paths:
            - ./dumps/*
        when: always

    script:
    # install
    - pip install .

    # benchmark: note that we have mounted the datasets at /mnt/datasets/d3m_datasets
    - mkdir dumps
    - python tools/benchmark/run_benchmark.py --datasets /mnt/datasets/d3m_datasets --id mini --exp dump --exp_arguments 300 --output dumps

openml-benchmark:
    image:
        name: registry.gitlab.com/einblick/alpine-meadow:latest
        entrypoint: [""]

    stage: scheduled

    only:
        variables:
            - $AM_TASK == 'OPENML_BENCHMARK'

    variables:
        GIT_SUBMODULE_STRATEGY: recursive

    artifacts:
        paths:
            - ./outputs/*
        when: always

    script:
    # install
    - pip install .
    - pip install -r tools/openml/requirements.txt

    # benchmark
    - mkdir outputs
    - python tools/openml/run_openml_benchmark.py --benchmark OpenML-CC18 --api_key $OPENML_API_KEY --time 60 --output_dir outputs --limit 30 --num_reps 1
